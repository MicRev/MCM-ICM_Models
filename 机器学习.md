# Logstics回归 与 损失函数

## Sigmoid函数推导
构建函数：
$$\log\frac{P}{1-P}=w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{2}=w^{T}x+b$$

令$w^{T}x+b=z$，可以得到 $\log\frac{P}{1-P}=z$ 进而推导出 $P=\frac{1}{1-e^{z}}$ ，得到Sigmoid函数

> **Sigmoid函数：** 
>$$\sigma(z) = \frac{1}{1-e^{-z}}$$

$$
    \hat{y}=\frac{1}{1-e^{-z}}（激活函数）
$$



## 损失函数
现在我们有了一套参数$w$ 和 $b$ 联系了$x$ 和 $\hat{y}$，没有调整的参数会使输出的$\hat{y}$ 和 $y$差距很大，我们希望 $\hat{y} \approx{y}$。

为此，引入损失函数$L(\hat{y},y)$。

损失函数是每个样本预测值和真实值的差值，而成本函数是所有损失函数的平均值。一般而言，损失函数越低，所建立的模型所提供的结果越好，所以损失函数被用于评估模型的性能，我们要让损失函数最小化

回归损失函数有多种，本模型采用对数损失，进而得到成本函数：

$$
    L(\hat{y},y) = -(y\ln{\hat{y}}+(1-y)\ln(1-\hat{y}))
$$
成本函数：
$$
    J(w,b)=\frac{1}{m}\sum_{1}^{m}L(\hat{y}^{(i)},y^{(i)})
$$
$$
    =\sum_{1}^{m}(y^{(-i)}ln\hat{y}^{(i)}+(1-y)\ln(1-\hat{y}))
$$
当 $y=1,L(\hat{y},y)=-ln\hat{y},\hat{y}$越大越接近1，$L(\hat{y},y)$越小

当 $y=0,L(\hat{y},y)=-ln(1-\hat{y}),\hat{y}$越小越接近0，$L(\hat{y},y)$越小

## 梯度下降与导数
梯度下降法的计算过程就是沿梯度下降的方向求解最小值

其迭代公式为
$$
a_{k+1}=a_k-\alpha\frac{dJ(a)}{da}
$$
代入本模型可以得到更新公式：
$$
w=w-\alpha\frac{dJ(w)}{dw}
$$
$\alpha$为学习速率（超参数）
当$w$大于最优解时，这点对应的导数$\frac{dJ(w)}{dw}>0$，所以根据更新公式，学习速率大于零，原有$w$减去一个正值，新的$w$会变小。

当$w$小于最优解时，这点对应的导数$\frac{dJ(w)}{dw}<0$，所以根据更新公式，学习速率大于零，原有$w$减去一个负值，新的$w$会变大。

**如下表达中，利用$a$ 代表 $\hat{y}$**

我们要利用梯度下降的方法找到能使成本函数$L(a,y)$达到最小值的参数$w、b$
$$
dw=\frac{\partial{L(a,y)}}{\partial{w}}=\frac{\partial{L(a,y)}}{\partial{a}}\frac{da}{dz}\frac{\partial{z}}{\partial{w}}
$$
$$
db=\frac{\partial{L(a,y)}}{\partial{b}}=\frac{\partial{L(a,y)}}{\partial{a}}\frac{da}{dz}\frac{\partial{z}}{\partial{b}}
$$
利用$dw$和$db$表示两个参数对成本函数的斜率

我们通过一定的数学方法，可以得到如下本模型需要的表达式：
$$
dz=a-y \qquad dw=xdz \qquad db=dz
$$

## 前向传播与反向传播
**前向传播**：将上一式子的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止。

**反向传播**：根据损失函数$L(a,y)$来反方向计算$a、z、w、b$的偏导数（梯度），从而更新参数。

### **前向传播 初始化参数：**
$$J=0，dw_{1}=0,dw_{2}=0,db=0$$
遍历样本：
$$
z^{(i)}=(w^{T}x^{(i)}+b)
$$
$$a^{(i)}=\sigma(z^{(i)})$$
$$L^{(i)}=-(y^{(i)}lna^{(i)}+(1-y^{(i)})ln(1-a^{(i)}))$$
### **反向传播**
$$dz^{(i)}=a^{(i)}-y^{(i)}$$
$$dw_1=\frac{1}{m}\sum_{1}^{m}x_1^{(i)}dz^{(i)}$$
$$dw_{2}=\frac{1}{m}\sum_{1}^{m}x_2^{(i)}dz^{(i)}$$
$$db=\frac{1}{m}dz^{(i)}$$
### **更新参数**
$$w_1=w_1-\alpha dw_1$$
$$w_2=w_2-\alpha dw_2$$
$$b=b-\alpha db$$
```py
#参数更新与梯度输出
def optimize(w,b,X,Y,num_item,learning_rate):
    costs=[]
    for i in range(num_item):
        grads,cost=propagate(w,b,X,Y)
        dw=grads['dw']
        db=grads['db']
        w=w - np.dot(learning_rate,dw)
        b=b - np.dot(learning_rate,db)
        if i%100==0:
            costs.append(cost)
            print(cost)
```

## 向量化Logistic回归的梯度输出
利用矩阵简化运算：
X矩阵维度为$(n_x,m)$是由一个个列向量样本横向堆叠起来，所以列数为样本数$m$，行数为特征值数$n_x$
$$
X=\left[\begin{matrix}
    | & | &&& &|\\
    | & | &&& &|\\
    x^{(1)} & x^{(2)} &.&.&.&x^{(m)}\\
    | & | &&& &|\\
    | & | &&& &|\\
\end{matrix}\right]
$$
$w$向量表示成一个列向量，行数是$n_x$；$w$向量与$X$向量维度要匹配
；$b$可以是一个实数，在Python中会有广播功能自动生成对应维度的矩阵。
$$
w=\left[\begin{matrix}
    w_1 \\
    w_2\\
    .\\
    . \\
    w_n \\
\end{matrix}\right]\qquad\qquad b=b
$$
### 向量化前向传播
回顾Logstics回归
$$
Z=w^T+b(Z是我们定义出的变量)
$$
经过矩阵运算我们可以得到用行向量表示的$Z$和$A$，方便我们用编程实现
$$Z=\left[\begin{matrix}
    Z^{(1)} &Z^{(2)} &...&Z^{(n)}\\
\end{matrix}\right]$$
$$A=\left[\begin{matrix}
    \sigma(Z^{(1)}) &\sigma(Z^{(2)}) &...&\sigma(Z^{(n)})\\
\end{matrix}\right]$$

### 向量化反向传播
参数更新
$$dz^{(n)}=a^{(n)}-y^{(n)}$$

$$dZ=A-Y=\left[\begin{matrix}
    a^{(1)}-y^{(1)} &a^{(2)}-y^{(2)} &...&a^{(n)}-y^{(n)}\\
\end{matrix}\right]$$

$$dw=\frac{1}{m}\sum_1^mx^{(i)}dz^{(i)}
=\frac{1}{m}XdZ^{T}
$$

$$db=\frac{1}{m}\sum_1^mdz^{(i)}$$
```py
#numpy编程实现
dw = (1/m) * np.dot(X, (A-Y).T)
db = (1/m) * np.sum(A-Y)
```
### 全代码练习
Logstics回归是一种二元判断的机器学习方法
如下代码实现了通过数据集分子判断有机物分子是否有光伏特性
```py
#库的使用与数据集的下载
import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect
import matplotlib.pyplot as plt
IPythonConsole.ipython_useSVG=True  

#!wget https://dp-public.oss-cn-beijing.aliyuncs.com/community/AI4EC/opv_train.csv
#!wget https://dp-public.oss-cn-beijing.aliyuncs.com/community/AI4EC/opv_test.csv
```
```py
#从数据集中读取数据 返回XY（拆分成训练集与测试集）
trainset=pd.read_csv("F:\Python try\opv_train.csv")
testset=pd.read_csv("F:\Python try\opv_test.csv")
X_train=trainset.iloc[:,0]
Y_train=trainset.iloc[:,1]
X_test=testset.iloc[:,0]
Y_test=testset.iloc[:,1]
```
将有机分子利用SMILES表达式转换为具有2048个变量的列表（每个变量为1或0），便于后续的矩阵运算和可视化表达
```py
def get_morgan(smis):
    fps=[]
    for smi in smis:
        mol = Chem.MolFromSmiles(smi, sanitize=True)
        fp = GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
        fp = fp.ToBitString()
        fp = [int(item) for item in list(fp)]
        fps.append(fp)
    return fps
```
```py
#将列表转换为数组，并改变形状为1行n列的矩阵（二维）
train_set_x=np.array(get_morgan(X_train)).T
train_set_y=np.array(Y_train).reshape((1,Y_train.shape[0]))
test_set_x=np.array(get_morgan(X_test)).T
test_set_y=np.array(Y_test).reshape((1,Y_test.shape[0]))
print(train_set_x.shape)

#可视化分子 观察分子结构
mols=[]
for smi in X_train:
        mol = Chem.MolFromSmiles(smi, sanitize=True)
        mols.append(mol)
```
构建Logstics回归使用的函数与变量：
```py
#构造sigmod函数
def sigmoid(z):
    a= 1 / (1 + np.exp(-z))
    return a
```
```py
#初始化参数w，b
def initialize_with_zeros(dim):
    np.random.seed(2)  #调用随机种子
    w=np.zeros((dim,1))
    b=0
    #利用断言判断判断数组形状和变量数据类型是否正确
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
    return w,b
```
```py
#前向传播与反向传播 loss= −(𝑦 ln⁡𝑎+(1−𝑦)ln⁡[(1−𝑎)])/m
def propagate(w,b,X,Y):
    m=X.shape[1]
    z= np.dot(w.T,X) + b
    A= sigmoid(z)
    #损失函数
    cost = -(np.dot(Y,np.log(A).T) + np.dot(1-Y,np.log(1-A).T)) / m
    #反向传播参数
    dw=  np.dot(X,(A-Y).T)/m#x*dz
    db=  np.sum(A-Y)#dz
    grads={"dw":dw,"db":db}
    #squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉
    cost=np.squeeze(cost)
    return grads,cost  #返回参数字典与损失函数

#参数更新与梯度输出
def optimize(w,b,X,Y,num_item,learning_rate):
    costs=[]
    for i in range(num_item):
        grads,cost=propagate(w,b,X,Y)
        dw=grads['dw']
        db=grads['db']
        w=w - np.dot(learning_rate,dw)
        b=b - np.dot(learning_rate,db)
        if i%100==0:
            costs.append(cost)
            print(cost) #每100次迭代，输出一次损失值
    
    params={'w':w,'b':b}
    grads={'dw':dw,'db':db}
    return params,grads,costs

#迭代后得到预测模型
def prediction(w,b,X):
    m=X.shape[1]
    Y_pred=np.zeros((1,m))
    A=sigmoid(np.dot(w.T,X) + b)
    for i in range(A.shape[1]):
        if A[0][i]<=0.5:
            A[0][i]=0
        else:
            A[0][i]=1
    Y_pred=A
    return Y_pred
```
```py
#构建模型函数
def model(X_train,X_test,Y_train,Y_test,num_item=5000,learning_rate=0.005):
    #根据X_train初始化w,b（权值与偏值）
    w,b=initialize_with_zeros(X_train.shape[0])
    #根据迭代次数和学习率得到更新参数与梯度
    parameters,grads,costs=optimize(w,b,X_train,Y_train,num_item,learning_rate)
    #预测的模型参数与预测值Y
    w=parameters['w']
    b=parameters['b']
    Y_prediction_test=prediction(w,b,X_test)
    Y_prediction_train=prediction(w,b,X_train)
    #输出准确率
    print('train accuracy:{}%'.format(100-np.mean(np.abs(Y_prediction_train-train_set_y)*100)))
    print('test accuracy:{}%'.format(100-np.mean(np.abs(Y_prediction_test-test_set_y)*100)))    
    
    d={'costs':costs,
       "Y_prediction_test":Y_prediction_test,
       "Y_prediction_train":Y_prediction_train,
       "w":w,
       "b":b,
       'learning_rate':learning_rate,
       "num_item":num_item
       }
    
    return d
```
最后的模型输出
```py
#模型计算过程
d=model(train_set_x,test_set_x,train_set_y,test_set_y)
costs=np.squeeze(d['costs'])
#作图
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('num_item per 100')
plt.title("learing rate = "+str(d['learning_rate']))
plt.show()

print(d['Y_prediction_test'])
print(Y_test)
```


