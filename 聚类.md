## K-Means

### 适用场景

无监督学习。在未标记的数据集中，指定分类的类别数k，将数据集按最近邻原则分为k类。

### 实现细节

- 指定需要分类的类别数k
- 随机选择k个点作为聚类中心
- 计算各点到各个聚类中心k的距离，找到距离最短的聚类中心，将所有选择这一聚类中心的点分为同一类
- 将每一类的数据点取平均值，作为新的聚类中心，重复上一步，直到旧聚类中心和新聚类中心的距离小于某一值
- 得到k个聚类中心和k组数据
  
为了得到最合理的聚类中心数，可以计算在各个分类中心数目下各个类的畸变程度。

假设 $n$ 个样本被分到 $K$ 个类中，$C_k$ 表示第 $k$ 个类，且该类的重心为 $\mu_k $ ，该类的畸变程度定义为

$$
\sum_{i \in C_k} |x_i - \mu_k|^2
$$

其中 $|x - \mu_k|$ 为某范数。总畸变程度（或聚合系数）：

$$
J = \sum_k^K \sum_{i \in C_k} |x_i - \mu_k|^2
$$

将聚类的类别数K为横坐标，J为纵坐标作图，得到折线图。若在折线图中，在K达到某一值后，J的下降速度明显放缓，则选择该K值作为聚类中心数。

### 所需假设

- 数据聚集在某一聚类中心周围，每一类数据都呈近圆形分布（或近方形、近椭圆曲线，取决于所使用的距离的定义）
- 所有变量的量纲相同，否则距离的计算没有意义
- 聚类中心数已知

### 可能风险

对初值敏感、对孤立点数据敏感

## K-Means ++

### 适用场景

解决K-Means可能的风险。

### 实现细节

基本与K-Means 相同，但在初值选取方面不同。K-Means++ 选择的初值尽可能远。

- 随机选取第一个聚类中心
- 计算每个样本与每一个中心的最短距离，距离越大，被选作下一个聚类中心的概率越大。
- 用按概率抽取下一个聚类中心
- 直到选出k个聚类中心

## 系统（层次）聚类

### 适用场景

希望得到系统的聚类谱系图

### 实现细节

反复计算两数据点之间的距离，将相聚最短的两个点合并，重复至所有数据点合并为一类，可以生成聚类谱系图

## DBSCAN

### 适用场景

DBSCAN（Density-Based spatial clustering of application with noise）算法，聚类前不需要给定聚类的类别数。利用基于密度聚合的概念，可以在有噪音的空间中发现任意形状的聚类簇。

### 实现细节

DBSCAN为每一个点选定了一个 $\epsilon$ 邻域，并以该邻域内点的数量将数据点分为三类

**核心点**：$\epsilon$ 邻域内的点的数量大于限度（$Minpts$）
**边界点**：$\epsilon$ 邻域内的点的数量小于限度（$Minpts$），但在核心点的邻域内
**噪音点**：$\epsilon$ 邻域内的点的数量小于限度（$Minpts$），且不在任何一核心点的邻域内

### 所需假设

- 数据点展现出明显的按密度分类的性质，形状有明显的非近圆性。
- 不同类数据点，甚至相同类数据点中，点的密度分布是均一的。

### 可能风险

- $\epsilon$ 和 $Minpts$ 选定可能不合理，导致错误的分类结果，而此参数的确定比较困难
- 数据量大时，计算密度的复杂度大